---
title: cbo
date: 2023-10-25 12:04:32
mathjax: true
tags:
- paper
- 数学
---
# 论文1
## 问题
论文[a consesus-based global optimization method for high dimensional machine learning problems](引用资源/论文)

针对无约束优化问题
$$\begin{equation} \label{eq1}
x^{*} = arg \min_{x \in R^{d}} L(x) 
\end{equation}$$

其中有$L(x^{*}) >0$

对于$N-$粒子系统$X^{j}, j = 1,\dots, N$
$$\begin{align}
dX^{j} &= -\lambda(X^{j} - \bar{x}^{*})H^{\varepsilon }(L(X^{j})-L(\bar{x}^{*}))\mathrm{d}t  + \sigma \left |X^{j} - \bar{x}^{*}  \right |dW^{j}  \label{eq2} \\
\bar{x}^{*} &= \frac{1}{\sum_{j=1}^{N}e^{-\beta L(X^{j}) } } \sum_{j=1}^{N} X^{j} e^{-\beta L(X^{j})} \label{eq3} 
\end{align}$$

对$\eqref{eq2}$的二阶矩求导
$$\begin{equation} \label{eq4}
\frac{\mathrm{d}   }{\mathrm{d} t} E\left | X-a \right|^{2} = (-2 \lambda + \sigma ^{2}d)E\left | X-a \right|^{2}  
\end{equation}$$
由$\eqref{eq4}$可以看到为了保证收敛性我们要保持$d, \lambda, \sigma$的关系。

## new method
$$\begin{equation} \label{eq5}
\begin{aligned}
dX^{j} &= -\lambda(X^{j} - \bar{x}^{*})H^{\varepsilon }(L(X^{j})-L(\bar{x}^{*}))\mathrm{d}t  + \sigma \sum_{k=1}^{d} ( X^{j} - \bar{x}^{*} )_{k} dW_{k} \bar{\epsilon}_{k}   \\
\bar{x}^{*} &= \frac{1}{\sum_{j=1}^{N}e^{-\beta L(X^{j}) } } \sum_{j=1}^{N} X^{j} e^{-\beta L(X^{j})} 
\end{aligned}
\end{equation}$$
同样的
$$\begin{equation} \label{eq6}
\frac{\mathrm{d}   }{\mathrm{d} t} E\left | X-a \right|^{2} = (-2 \lambda + \sigma ^{2})E\left | X-a \right|^{2}  
\end{equation}$$

## 算法
1. 按照同分布产生$N$个粒子。按下面的步骤来更新这$N$个粒子：**每次更新前**先划分成$q$个粒子群。
2. 对每个粒子群的粒子计算损失函数$L^{j}$（可以用SGD近似，即$\hat{L}^{j}$）
3. 对每个粒子群迭代其中心$\bar{x}^{*}_{k, \theta}$,$k$是迭代次数，$\theta \in range(q)$。
4. 更新$X^{j}$（可以选择全部更新，或者一个粒子群更新）

## cbo for distribute optimization
分布式优化中cbo作为一种同步的方法
{% asset_img img01.png %}

# 论文2
论文[Consensus-based optimization via jump-diffusion stochastic differential equations](引用资源/论文) 

改了一个扩散项。